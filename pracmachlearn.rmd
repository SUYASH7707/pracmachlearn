# practical Machine Learning Coursera Peer Assessment 
 ## Summary 
This report uses machine learning algorithms to predict the way during which users of exercise devices exercise.
 ### Background Using devices like Jawbone Up, Greek deity FuelBand, and Fitbit it's currently attainable to gather an outsized quantity of knowledge concerning personal activity comparatively inexpensively. These form of devices area unit a part of the quantified self movement â€“ a gaggle of enthusiasts UN agency take measurements concerning themselves frequently to enhance their health, to search out patterns in their behavior, or as a result of they're technical school geeks. One issue that individuals frequently do is quantify what proportion of a specific activity they are doing, however they seldom quantify however well they are doing it. during this project, your goal are going to be to use knowledge from accelerometers on the belt, forearm, arm, and dumbell of half dozen participants. They were asked to perform weight lifts properly and incorrectly in five alternative ways. a lot of info is accessible from the web site [here:](http://groupware.les.inf.puc-rio.br/har) (see the section on the burden Lifting Exercise Dataset). ### Data The coaching knowledge for this project area unit accessible here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv The check knowledge area unit accessible here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv ### Set the work atmosphere and knitr choices ```r rm(list=ls(all=TRUE)) #start with empty space startTime <- Sys.time() library(knitr) opts_chunk$set(echo = TRUE, cache= TRUE, results = 'hold') ``` ### Load libraries and Set Seed Load all libraries used, and setting seed for dependability. *Results Hidden, Warnings FALSE and Messages FALSE* ```r library(ElemStatLearn) library(caret) library(rpart) library(randomForest) library(RCurl) set.seed(2014) ``` ### Load and prepare the information and pack up the information Load and prepare the information ```r trainingLink <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv") pml_CSV <- scan.csv(text = trainingLink, header=TRUE, sep=",", na.strings=c("NA","")) pml_CSV <- pml_CSV[,-1] # take away the primary column that represents a ID Row ``` ### knowledge Sets Partitions Definitions Create knowledge partitions of coaching and validatory knowledge sets. ```r inTrain = createDataPartition(pml_CSV$classe, p=0.60, list=FALSE) training = pml_CSV[inTrain,] validating = pml_CSV[-inTrain,] # variety of rows and columns of knowledge within the coaching set dim(training) # variety of rows and columns of knowledge within the validatory set dim(validating) ``` ``` ## [1] 11776 159 ## [1] 7846 159 ``` ## knowledge Exploration and cleansing Since we elect a random forest model and that we have a knowledge set with too several columns, initial we tend to check if we've got several issues with columns while not knowledge. So, take away columns that have but hour of knowledge entered. ```r # variety of cols with but hour of knowledge sum((colSums(!is.na(training[,-ncol(training)])) < zero.6*nrow(training))) ``` [1] 100 ```r # apply our definition of take away columns that the majority does not have knowledge, before its apply to the model. Keep <- c((colSums(!is.na(training[,-ncol(training)])) >= zero.6*nrow(training))) training <- coaching[,Keep] validating <- validatory[,Keep] # variety of rows and columns of knowledge within the final coaching set dim(training) ``` [1] 11776 fifty nine ```r # variety of rows and columns of knowledge within the final validatory set dim(validating) ``` [1] 7846 fifty nine ## Modeling In random forests, there's no want for cross-validation or a separate check set to urge associate unbiased estimate of the check set error. it's calculable internally, throughout the execution. So, we tend to proceed with the coaching the model (Random Forest) with the coaching knowledge set. ```r model <- randomForest(classe~.,data=training) print(model) ``` ``` ## ## Call: ## randomForest(formula = classe ~ ., knowledge = training) ## form of random forest: classification ## variety of trees: five hundred ## No. of variables tried at every split: seven ## ## OOB estimate of error rate: zero.19% ## Confusion matrix: ## A B C D E category.error ## A 3348 zero zero zero zero zero.000000000 ## B three 2276 zero zero zero zero.001316367 ## C zero nine 2044 one zero zero.004868549 ## D zero zero five one924 1 zero.003108808 ## E zero zero zero three 2162 zero.001385681 ``` ### Model appraise And proceed with the verification of variable importance measures as made by random Forest: ```r importance(model) ``` ``` ## MeanDecreaseGini ## user_name ninety.3730521 ## raw_timestamp_part_1 932.6373613 ## raw_timestamp_part_2 eleven.2278639 ## cvtd_timestamp 1417.2774858 ## new_window zero.2375014 ## num_window 538.0882905 ## roll_belt 547.0536679 ## pitch_belt 289.9572495 ## yaw_belt 342.2318442 ## total_accel_belt a hundred and ten.0633448 ## gyros_belt_x thirty-nine.5889951 ## gyros_belt_y forty five.1635594 ## gyros_belt_z 116.7332576 ## accel_belt_x sixty five.1914831 ## accel_belt_y seventy one.6575192 ## accel_belt_z 174.5775863 ## magnet_belt_x 109.5946175 ## magnet_belt_y 198.2364446 ## magnet_belt_z 174.1100246 ## roll_arm 123.8385402 ## pitch_arm fifty six.7411710 ## yaw_arm eighty.6033881 ## total_accel_arm twenty six.3682367 ## gyros_arm_x forty two.2808067 ## gyros_arm_y forty one.6757042 ## gyros_arm_z nineteen.4557642 ## accel_arm_x ninety four.6270535 ## accel_arm_y fifty four.4922538 ## accel_arm_z forty.7576689 ## magnet_arm_x a hundred and five.2342845 ## magnet_arm_y seventy nine.6373607 ## magnet_arm_z fifty seven.7204415 ## roll_dumbbell 197.6213608 ## pitch_dumbbell seventy five.0525013 ## yaw_dumbbell 104.9213658 ## total_accel_dumbbell 112.5343776 ## gyros_dumbbell_x forty two.7839013 ## gyros_dumbbell_y a hundred and ten.7356305 ## gyros_dumbbell_z twenty five.1911639 ## accel_dumbbell_x 126.2760046 ## accel_dumbbell_y 183.1386045 ## accel_dumbbell_z one hundred forty.3221880 ## magnet_dumbbell_x 234.3036947 ## magnet_dumbbell_y 321.8106105 ## magnet_dumbbell_z 299.7706537 ## roll_forearm 232.9445408 ## pitch_forearm 293.0121796 ## yaw_forearm fifty nine.1226542 ## total_accel_forearm thirty three.2545324 ## gyros_forearm_x twenty four.9673052 ## gyros_forearm_y forty one.4192787 ## gyros_forearm_z twenty six.9075827 ## accel_forearm_x 133.6714294 ## accel_forearm_y forty five.3258310 ## accel_forearm_z ninety six.1075329 ## magnet_forearm_x seventy six.6923241 ## magnet_forearm_y seventy six.9926445 ## magnet_forearm_z ninety seven.9443069 ``` Now we tend to appraise our model results through confusion Matrix. ```r confusionMatrix(predict(model,newdata=validating[,-ncol(validating)]),validating$classe) ``` ``` ## Confusion Matrix and Statistics ## ## Reference ## Prediction A B C D E ## A 2231 zero zero zero zero ## B one 1518 five zero zero ## C zero zero one362 1 zero ## D zero zero one one285 1 ## E zero zero zero zero 1441 ## ## Overall Statistics ## ## Accuracy : zero.9989 ## ninety fifth CI : (0.9978, 0.9995) ## No info Rate : zero.2845 ## P-Value [Acc > NIR] : < two.2e-16 ## ## letter : zero.9985 ## Mcnemar's check P-Value : sodium ## ## Statistics by Class: ## ## Class: A Class: B Class: C Class: D Class: E ## Sensitivity zero.9996 1.0000 0.9956 0.9992 0.9993 ## Specificity one.0000 0.9991 0.9998 0.9997 1.0000 ## Pos Pred worth one.0000 0.9961 0.9993 0.9984 1.0000 ## Neg Pred worth zero.9998 1.0000 0.9991 0.9998 0.9998 ## Prevalence zero.2845 0.1935 0.1744 0.1639 0.1838 ## Detection Rate zero.2843 0.1935 0.1736 0.1638 0.1837 ## Detection Prevalence zero.2843 0.1942 0.1737 0.1640 0.1837 ## Balanced Accuracy zero.9998 0.9995 0.9977 0.9995 0.9997 ``` And confirmed the accuracy at validatory knowledge set by calculate it with the formula: ```r accuracy <-c(as.numeric(predict(model,newdata=validating[,-ncol(validating)])==validating$classe)) accuracy <-sum(accuracy)*100/nrow(validating) ``` Model Accuracy as tested over Validation set = **99.9%**. ### Model check Finally, we tend to proceed with predicting the new values within the testing csv provided, initial we tend to apply identical knowledge cleansing operations thereon and obligate all columns of testing knowledge set for identical category of previous knowledge set. #### obtaining Testing Dataset ```r testingLink <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv") pml_CSV <- scan.csv(text = testingLink, header=TRUE, sep=",", na.strings=c("NA","")) pml_CSV <- pml_CSV[,-1] # take away the primary column that represents a ID Row pml_CSV <- pml_CSV[ , Keep] # Keep identical columns of testing dataset pml_CSV <- pml_CSV[,-ncol(pml_CSV)] # take away the matter ID # Apply identical Transformations and obligate Testing Dataset # obligate testing dataset to same category and strucuture of coaching dataset testing <- rbind(training[100, -59] , pml_CSV) # Apply the ID Row to row.names and a hundred for dummy row from testing dataset row.names(testing) <- c(100, 1:20) ``` #### Predicting with testing dataset ```r predictions <- predict(model,newdata=testing[-1,]) print(predictions) ``` ``` ## one two three four five half dozen seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty ## B A B A A E D B A A B C B A E E A B B B ## Levels: A B C D E ``` #### the subsequent operate to form the files to answers the Prediction Assignment Submission: ```r pml_write_files = function(x)file name = paste0(pathAnswers,"answers/problem_id_",i,".txt") write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE) } } pml_write_files(predictions) #get the time ``` ```r endTime <- Sys.time() ``` The analysis was completed on Tue oct twenty one 12:34:05 PM 2020 in sixty four seconds
