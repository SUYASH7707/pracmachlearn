# sensible Machine Learning Coursera Peer Assessment Monday, July 21, 2014 ## Summary This report uses machine learning algorithms to predict the style within which users of exercise devices exercise. ### Background Using devices like Jawbone Up, Greek deity FuelBand, and Fitbit it's currently attainable to gather an oversized quantity of information regarding personal activity comparatively inexpensively. These style of devices ar a part of the quantified self movement â€“ a gaggle of enthusiasts WHO take measurements regarding themselves often to enhance their health, to seek out patterns in their behavior, or as a result of they're technical school geeks. One factor that folks often do is quantify what quantity of a selected activity they are doing, however they seldom quantify however well they are doing it. during this project, your goal are going to be to use knowledge from accelerometers on the belt, forearm, arm, and dumbell of half dozen participants. They were asked to perform free weight lifts properly and incorrectly in five other ways. a lot of data is out there from the web site [here:](http://groupware.les.inf.puc-rio.br/har) (see the section on the burden Lifting Exercise Dataset). ### Data The coaching knowledge for this project ar on the market here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv The take a look at knowledge ar on the market here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv ### Set the work atmosphere and knitr choices ```r rm(list=ls(all=TRUE)) #start with empty space startTime <- Sys.time() library(knitr) opts_chunk$set(echo = TRUE, cache= TRUE, results = 'hold') ``` ### Load libraries and Set Seed Load all libraries used, and setting seed for dependability. *Results Hidden, Warnings FALSE and Messages FALSE* ```r library(ElemStatLearn) library(caret) library(rpart) library(randomForest) library(RCurl) set.seed(2014) ``` ### Load and prepare the info and finish off the info Load and prepare the info ```r trainingLink <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv") pml_CSV <- scan.csv(text = trainingLink, header=TRUE, sep=",", na.strings=c("NA","")) pml_CSV <- pml_CSV[,-1] # take away the primary column that represents a ID Row ``` ### knowledge Sets Partitions Definitions Create knowledge partitions of coaching and verificatory knowledge sets. ```r inTrain = createDataPartition(pml_CSV$classe, p=0.60, list=FALSE) training = pml_CSV[inTrain,] validating = pml_CSV[-inTrain,] # range of rows and columns of information within the coaching set dim(training) # range of rows and columns of information within the verificatory set dim(validating) ``` ``` ## [1] 11776 159 ## [1] 7846 159 ``` ## knowledge Exploration and cleanup Since we elect a random forest model and that we have a knowledge set with too several columns, 1st we have a tendency to check if we've several issues with columns while not knowledge. So, take away columns that have but hour of information entered. ```r # range of cols with but hour of information sum((colSums(!is.na(training[,-ncol(training)])) < zero.6*nrow(training))) ``` [1] 100 ```r # apply our definition of take away columns that the majority does not have knowledge, before its apply to the model. Keep <- c((colSums(!is.na(training[,-ncol(training)])) >= zero.6*nrow(training))) training <- coaching[,Keep] validating <- verificatory[,Keep] # range of rows and columns of information within the final coaching set dim(training) ``` [1] 11776 fifty nine ```r # range of rows and columns of information within the final verificatory set dim(validating) ``` [1] 7846 fifty nine ## Modeling In random forests, there's no want for cross-validation or a separate take a look at set to urge AN unbiased estimate of the take a look at set error. it's calculable internally, throughout the execution. So, we have a tendency to proceed with the coaching the model (Random Forest) with the coaching knowledge set. ```r model <- randomForest(classe~.,data=training) print(model) ``` ``` ## ## Call: ## randomForest(formula = classe ~ ., knowledge = training) ## style of random forest: classification ## range of trees: five hundred ## No. of variables tried at every split: seven ## ## OOB estimate of error rate: zero.19% ## Confusion matrix: ## A B C D E category.error ## A 3348 zero zero zero zero zero.000000000 ## B three 2276 zero zero zero zero.001316367 ## C zero nine 2044 one zero zero.004868549 ## D zero zero five one924 1 zero.003108808 ## E zero zero zero three 2162 zero.001385681 ``` ### Model assess And proceed with the verification of variable importance measures as created by random Forest: ```r importance(model) ``` ``` ## MeanDecreaseGini ## user_name ninety.3730521 ## raw_timestamp_part_1 932.6373613 ## raw_timestamp_part_2 eleven.2278639 ## cvtd_timestamp 1417.2774858 ## new_window zero.2375014 ## num_window 538.0882905 ## roll_belt 547.0536679 ## pitch_belt 289.9572495 ## yaw_belt 342.2318442 ## total_accel_belt a hundred and ten.0633448 ## gyros_belt_x thirty-nine.5889951 ## gyros_belt_y forty five.1635594 ## gyros_belt_z 116.7332576 ## accel_belt_x sixty five.1914831 ## accel_belt_y seventy one.6575192 ## accel_belt_z 174.5775863 ## magnet_belt_x 109.5946175 ## magnet_belt_y 198.2364446 ## magnet_belt_z 174.1100246 ## roll_arm 123.8385402 ## pitch_arm fifty six.7411710 ## yaw_arm eighty.6033881 ## total_accel_arm twenty six.3682367 ## gyros_arm_x forty two.2808067 ## gyros_arm_y forty one.6757042 ## gyros_arm_z nineteen.4557642 ## accel_arm_x ninety four.6270535 ## accel_arm_y fifty four.4922538 ## accel_arm_z forty.7576689 ## magnet_arm_x one zero five.2342845 ## magnet_arm_y seventy nine.6373607 ## magnet_arm_z fifty seven.7204415 ## roll_dumbbell 197.6213608 ## pitch_dumbbell seventy five.0525013 ## yaw_dumbbell 104.9213658 ## total_accel_dumbbell 112.5343776 ## gyros_dumbbell_x forty two.7839013 ## gyros_dumbbell_y a hundred and ten.7356305 ## gyros_dumbbell_z twenty five.1911639 ## accel_dumbbell_x 126.2760046 ## accel_dumbbell_y 183.1386045 ## accel_dumbbell_z one hundred forty.3221880 ## magnet_dumbbell_x 234.3036947 ## magnet_dumbbell_y 321.8106105 ## magnet_dumbbell_z 299.7706537 ## roll_forearm 232.9445408 ## pitch_forearm 293.0121796 ## yaw_forearm fifty nine.1226542 ## total_accel_forearm thirty three.2545324 ## gyros_forearm_x twenty four.9673052 ## gyros_forearm_y forty one.4192787 ## gyros_forearm_z twenty six.9075827 ## accel_forearm_x 133.6714294 ## accel_forearm_y forty five.3258310 ## accel_forearm_z ninety six.1075329 ## magnet_forearm_x seventy six.6923241 ## magnet_forearm_y seventy six.9926445 ## magnet_forearm_z ninety seven.9443069 ``` Now we have a tendency to assess our model results through confusion Matrix. ```r confusionMatrix(predict(model,newdata=validating[,-ncol(validating)]),validating$classe) ``` ``` ## Confusion Matrix and Statistics ## ## Reference ## Prediction A B C D E ## A 2231 zero zero zero zero ## B one 1518 five zero zero ## C zero zero one362 1 zero ## D zero zero one one285 1 ## E zero zero zero zero 1441 ## ## Overall Statistics ## ## Accuracy : zero.9989 ## ninety fifth CI : (0.9978, 0.9995) ## No data Rate : zero.2845 ## P-Value [Acc > NIR] : < two.2e-16 ## ## letter : zero.9985 ## Mcnemar's take a look at P-Value : sodium ## ## Statistics by Class: ## ## Class: A Class: B Class: C Class: D Class: E ## Sensitivity zero.9996 1.0000 0.9956 0.9992 0.9993 ## Specificity one.0000 0.9991 0.9998 0.9997 1.0000 ## Pos Pred price one.0000 0.9961 0.9993 0.9984 1.0000 ## Neg Pred price zero.9998 1.0000 0.9991 0.9998 0.9998 ## Prevalence zero.2845 0.1935 0.1744 0.1639 0.1838 ## Detection Rate zero.2843 0.1935 0.1736 0.1638 0.1837 ## Detection Prevalence zero.2843 0.1942 0.1737 0.1640 0.1837 ## Balanced Accuracy zero.9998 0.9995 0.9977 0.9995 0.9997 ``` And confirmed the accuracy at verificatory knowledge set by calculate it with the formula: ```r accuracy <-c(as.numeric(predict(model,newdata=validating[,-ncol(validating)])==validating$classe)) accuracy <-sum(accuracy)*100/nrow(validating) ``` Model Accuracy as tested over Validation set = **99.9%**. ### Model take a look at Finally, we have a tendency to proceed with predicting the new values within the testing csv provided, 1st we have a tendency to apply an equivalent knowledge cleanup operations thereon and squeeze all columns of testing knowledge set for an equivalent category of previous knowledge set. #### obtaining Testing Dataset ```r testingLink <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv") pml_CSV <- scan.csv(text = testingLink, header=TRUE, sep=",", na.strings=c("NA","")) pml_CSV <- pml_CSV[,-1] # take away the primary column that represents a ID Row pml_CSV <- pml_CSV[ , Keep] # Keep an equivalent columns of testing dataset pml_CSV <- pml_CSV[,-ncol(pml_CSV)] # take away the matter ID # Apply an equivalent Transformations and squeeze Testing Dataset # squeeze testing dataset to same category and strucuture of coaching dataset testing <- rbind(training[100, -59] , pml_CSV) # Apply the ID Row to row.names and a hundred for dummy row from testing dataset row.names(testing) <- c(100, 1:20) ``` #### Predicting with testing dataset ```r predictions <- predict(model,newdata=testing[-1,]) print(predictions) ``` ``` ## one two three four five half dozen seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty ## B A B A A E D B A A B C B A E E A B B B ## Levels: A B C D E ``` #### the subsequent operate to form the files to answers the Prediction Assignment Submission: ```r pml_write_files = function(x)computer file name = paste0(pathAnswers,"answers/problem_id_",i,".txt") write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE) } } pml_write_files(predictions) #get the time ``` ```r endTime <- Sys.time() ``` The analysis was completed on Tue Jul twenty one 12:34:05 PM
